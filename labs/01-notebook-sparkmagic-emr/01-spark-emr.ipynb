{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark on SageMaker Studio with EMR Cluster\n",
    "\n",
    "This notebook shows how to run PySpark code within a SageMaker Studio notebook by using an EMR cluster for executing jobs. For this example we use the **SparkMagic - PySpark** image and kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr connect --cluster-id j-3CPACJST7EJ2M --auth-type None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Scoped Dependencies\n",
    "Notebook-scoped libraries provide you the following benefits:\n",
    "\n",
    "* Runtime installation – You can import your favorite Python libraries from PyPI repositories and install them on your remote cluster on the fly when you need them. These libraries are instantly available to your Spark runtime environment. There is no need to restart the notebook session or recreate your cluster.\n",
    "* Dependency isolation – The libraries you install using EMR Notebooks are isolated to your notebook session and don’t interfere with bootstrapped cluster libraries or libraries installed from other notebook sessions. These notebook-scoped libraries take precedence over bootstrapped libraries. Multiple notebook users can import their preferred version of the library and use it without dependency clashes on the same cluster.\n",
    "* Portable library environment – The library package installation happens from your notebook file. This allows you to recreate the library environment when you switch the notebook to a different cluster by re-executing the notebook code. At the end of the notebook session, the libraries you install through EMR Notebooks are automatically removed from the hosting EMR cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upgrade pip\n",
    "\n",
    "In this cells we are updating the pip version for installing pyarrow module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.uninstall_package(\"pip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pyarrow\") # install pyarrow to run vectorized UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks python modules installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "s3_bucket = \"\"\n",
    "s3_processed_data_location = f\"s3a://{s3_bucket}/data/output/\" # location where spark will write the processed data for training\n",
    "\n",
    "object_name = \"LD2011_2014.csv\"\n",
    "\n",
    "s3_input_data_location = \"s3://{}/data/input/{}\".format(s3_bucket, object_name)\n",
    "schema = \"date TIMESTAMP, client STRING, value FLOAT\" # source data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all we need to preprocess the data with spark. We'll send to spark cluster the location of the input data, the S3 location of where we'd like the output to go, and the schema information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i s3_input_data_location -t str -n s3_input_data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i s3_processed_data_location -t str -n s3_processed_data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i schema -t str -n schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing with Apache Spark\n",
    "The input dataset comes in the following format:\n",
    "\n",
    "|    | date                | client   |   value |\n",
    "|---:|:--------------------|:---------|--------:|\n",
    "|  0 | 2011-01-01 00:15:00 | MT_001   |       0 |\n",
    "|  1 | 2011-01-01 00:30:00 | MT_001   |       0 |\n",
    "|  2 | 2011-01-01 00:45:00 | MT_001   |       0 |\n",
    "|  3 | 2011-01-01 01:00:00 | MT_001   |       0 |\n",
    "|  4 | 2011-01-01 01:15:00 | MT_001   |       0 |\n",
    "\n",
    "The first column contains the timestamp of the observation in 15 min increments. The `client` column uniquely identifies each timeseries (i.e. the customer), and the `value` column provides the electricity demand at that time\n",
    "\n",
    "For DeepAR we'll need to transform the timeseries data into a json lines format where each line contains a json object representing each client and having the following schema: <br>\n",
    "`{\"start\": ..., \"target\": [0, 0, 0, 0], \"dynamic_feat\": [[0, 1, 1, 0]], \"cat\": [0, 0]}` <br>\n",
    "We'll only use the `start` attribute which contains the start date for the timesries, the `target` attribute which contains the observations, and the `cat` attribute with which will encode each client as a category. DeepAR supports providing additional categorical and continuous features to improve the quality of the forecast\n",
    "\n",
    "Here we will read the data from S3, and then use a compination of PySpark and PandasUDFs to get the data into the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"date TIMESTAMP, client STRING, value FLOAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .read \\\n",
    "    .schema(schema) \\\n",
    "    .options(sep =',', header=True, mode=\"FAILFAST\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\") \\\n",
    "    .csv(s3_input_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample from 15min intervals to one hour to speed up training\n",
    "df = df \\\n",
    "    .groupBy(fn.date_trunc(\"HOUR\", fn.col(\"date\")).alias(\"date\"), fn.col(\"client\")) \\\n",
    "    .agg(fn.mean(\"value\").alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to Integer encode each client\n",
    "client_list = df.select(\"client\").distinct().collect()\n",
    "client_list = [rec[\"client\"] for rec in client_list]\n",
    "client_encoder = dict(zip(client_list, range(len(client_list)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_client_list = random.sample(client_list, 6)\n",
    "\n",
    "random_clients_pandas_df = df \\\n",
    "                            .where(fn.col(\"client\").isin(random_client_list)) \\\n",
    "                            .groupBy(\"date\") \\\n",
    "                            .pivot(\"client\").max().toPandas()\n",
    "\n",
    "random_clients_pandas_df.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_clients_pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating data for removing gaps. So for example if you have data that only comes in Monday to Friday (e.g. stock trading activity), we'd have to insert NaN data points to account for Saturdays and Sundays. A quick way to check if our data has any gaps is to aggregate by the day of the week. Running the commands below we can see that the difference between the count and the lowest count is 24 Hours which is ok as it just means that the last datapoint falls midweek. Also the counts match across all customers so it appears that this dataset does not have any gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_counts = df \\\n",
    "                .withColumn(\"dayofweek\", fn.dayofweek(\"date\")) \\\n",
    "                .groupBy(\"client\") \\\n",
    "                .pivot(\"dayofweek\") \\\n",
    "                .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_counts.show(5) # show aggregates for several clients\n",
    "weekday_counts.agg(*[fn.min(col) for col in weekday_counts.columns[1:]]).show() # show minimum counts of observations across all clients\n",
    "weekday_counts.agg(*[fn.max(col) for col in weekday_counts.columns[1:]]).show() # show maximum counts of observations across all clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = df.select(fn.min(\"date\").alias(\"date\")).collect()[0][\"date\"]\n",
    "test_start_date = \"2014-01-01\"\n",
    "end_date = df.select(fn.max(\"date\").alias(\"date\")).collect()[0][\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "train_data_tmp = df.where(fn.col(\"date\") < test_start_date)\n",
    "test_data_tmp = df.where(fn.col(\"date\") >= test_start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasUDFs require an output schema. This one matches the format required for DeepAR\n",
    "dataset_schema = StructType([StructField(\"target\", ArrayType(DoubleType())),\n",
    "                             StructField(\"cat\", ArrayType(IntegerType())),\n",
    "                             StructField(\"start\", StringType())\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(dataset_schema, PandasUDFType.GROUPED_MAP)\n",
    "def prep_deep_ar(df):\n",
    "    \n",
    "    df = df.sort_values(by=\"date\")\n",
    "    client_name = df.loc[0, \"client\"]\n",
    "    targets = df[\"value\"].values.tolist()\n",
    "    cat = [client_encoder[client_name]]\n",
    "    start = str(df.loc[0,\"date\"])\n",
    "    \n",
    "    return pd.DataFrame([[targets, cat, start]], columns=[\"target\", \"cat\", \"start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag so that _SUCCESS meta files are not written to S3\n",
    "# DeepAR actually skips these files anyway, but it's a good practice when using directories as inputs to algorithms\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "spark.conf.set(\"spark.hadoop.orc.overwrite.output.file\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_tmp.groupBy(\"client\").apply(prep_deep_ar)\n",
    "test_data = test_data_tmp.groupBy(\"client\").apply(prep_deep_ar)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "PySpark (SparkMagic)",
   "language": "python",
   "name": "pysparkkernel__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-sparkmagic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
