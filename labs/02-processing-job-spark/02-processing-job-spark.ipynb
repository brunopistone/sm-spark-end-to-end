{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing Job using PySpark\n",
    "\n",
    "This notebook will show you how can be used pyspark for processing data using the fully-managed service Amazon SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script was created to express what we saw in the previous exercise.\n",
    "## It will get the raw data from the turbine sensors, select some features, \n",
    "## denoise, normalize, encode and reshape it as a 6x10x10 tensor\n",
    "## This script is the entrypoint of the first step of the ML Pipelie: Data preparation\n",
    "!pygmentize ./processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define imports and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = \"sm-emr-sc-blog-06be16c8160f\"\n",
    "file_name = \"LD2011_2014.csv\"\n",
    "\n",
    "s3_input_file = \"s3://{}/data/input/{}\".format(s3_bucket_name, file_name)\n",
    "s3_output_path = \"s3://{}/data/output/\".format(s3_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=runtime_client,\n",
    "    default_bucket=s3_bucket_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PySpark Processor\n",
    "\n",
    "By using [SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html) we can interact with SageMaker Jobs by using the official SageMaker Containers.\n",
    "\n",
    "In the following example, we are creating a PySparkProcessor, which allows us to create a Amazon SageMaker Processing Job by using the SageMaker container with the PySpark modules configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"3.1\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Run SageMaker PySpark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./processing.py\",\n",
    "    arguments=[\n",
    "        \"--s3_input_file\",\n",
    "        s3_input_file,\n",
    "        \"--s3_output_path\",\n",
    "        s3_output_path\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/spark_execution/{}/spark_event_logs\".format(s3_bucket_name, timestamp_prefix),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override PySpark configuration in SageMaker Processing Jobs\n",
    "\n",
    "SageMaker Processing Jobs create the PySpark environment during the creation of the container itself.\n",
    "For overriding Spark configurations, such as SPARK_DRIVER_MEMORY and SPARK_EXECUTOR_MEMORY, we can provide a configuration file as input to the Processing Job itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define spark-defaults configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_DRIVER_MEMORY = \"2g\"\n",
    "SPARK_EXECUTOR_MEMORY = \"2g\"\n",
    "\n",
    "configurations = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\"spark.driver.memory\": SPARK_DRIVER_MEMORY, \"spark.executor.memory\": SPARK_EXECUTOR_MEMORY}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store configurations as file on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = \"sm-emr-sc-blog-06be16c8160f\"\n",
    "config_path = \"spark/configurations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.put_object(\n",
    "    Body=(bytes(json.dumps(configurations).encode('UTF-8'))),\n",
    "    Bucket=s3_bucket_name,\n",
    "    Key=config_path + \"/configuration.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SageMaker Processing Job by providing configuration file as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./processing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://{}/{}/configuration.json\".format(s3_bucket_name, config_path), \n",
    "            destination=\"/opt/ml/processing/input/conf\"),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_file\",\n",
    "        s3_input_file,\n",
    "        \"--s3_output_path\",\n",
    "        s3_output_path\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/spark_execution/{}/spark_event_logs\".format(s3_bucket_name, timestamp_prefix),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own Container\n",
    "\n",
    "It's possible to override the SageMaker PySpark container by creating your container starting from one of the public container imanges provided by SageMaker for Spark.\n",
    "\n",
    "You can find the full list of images here: https://github.com/aws/sagemaker-spark-container/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_image_uri = \"<ECR_IMAGE_URI>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    image_uri=ecr_image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(\n",
    "    arguments=[\n",
    "        \"--s3_input_file\",\n",
    "        s3_input_file,\n",
    "        \"--s3_output_path\",\n",
    "        s3_output_path\n",
    "    ],\n",
    "    logs=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
